defaults:
    - logger_cfgs: Tensorboard
    #- override hydra/launcher: submitit_slurm

experiment_name: train_cnn_14H

hydra:
    run:
        dir: ./results/train_titration_14H_cnn/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

    launcher:
        partition: gaia
        tasks_per_node: 2
        nodes: ${trainer_cfg.num_nodes}
        constraint: xeon-g6
        timeout_min: 10000
        additional_parameters:
            {'gres':'gpu:volta:2'}

model_cfg:
    _target_: src.models.CNN
    _recursive_: False
    enc_channels: [30, 32, 64]
    enc_kernel_sizes: [5, 5]
    enc_strides: [1, 1]
    enc_paddings: [0, 0]

    enc_hidden_sizes: [320, 160]
    mp_kernel_size: 2
    mp_stride: 2
    dropout: .15
    verbose: False
    optimizer_cfg:
        _target_: torch.optim.Adam
        lr: .0001
    inducing_points: null 

trainer_cfg:
    max_epochs: 5000
    gpus: 2
    num_nodes: 1
    distributed_backend: ddp

train_set_cfg:
    _target_: src.datasets.CovidDataset
    data_path: /home/gridsan/groups/ai4bio_shared/datasets/AAbio_data/covid/
    split: train
    chain: 14H
    average_replicates: True
    add_static_ends: True
    correction: replicate
    filter_nan: median
    collate_type: full_padded
    token_encode_type: one_hot
    feat_model: null
    
train_dataloader_cfg:
    batch_size: 32
    shuffle: True

callback_cfgs:
    early_stopping:
        _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: val_loss
        verbose: True
        patience: 20
        
checkpoint_callback_cfg:
    filepath: checkpoint/
    
val_set_cfg:
    _target_: ${train_set_cfg._target_}
    data_path: ${train_set_cfg.data_path}
    split: valid
    chain: ${train_set_cfg.chain}
    average_replicates: ${train_set_cfg.average_replicates}
    add_static_ends: ${train_set_cfg.add_static_ends}
    correction: ${train_set_cfg.correction}
    filter_nan: ${train_set_cfg.filter_nan}
    collate_type: ${train_set_cfg.collate_type}
    token_encode_type: ${train_set_cfg.token_encode_type}
    feat_model: null
    
val_dataloader_cfg:
    batch_size: ${train_dataloader_cfg.batch_size}
