defaults:
    - logger_cfgs: Tensorboard
    #- override hydra/launcher: submitit_slurm

experiment_name: "train_finetune_14L"
nodes: 1
strict_reload: False
reload_checkpoint_path: /home/li25662/AIforGreatGood/biotransfer/pretrained_models/pfam.ckpt # Full path to the pretrained langauge model checkpoint

hydra:
    run:
        dir: ./results/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
############

model_cfg:
    _target_: src.models.BertForValuePrediction
    downstream_dropout: 0.1
    downstream_hid_dim: 512
    lr: 2.79551209035589e-05
    loss_function: mse
    model_config_file: /home/li25662/AIforGreatGood/biotransfer/pretrained_models/bert_configs/config.json # full path to the bert config file
    warmup_steps: 10000

trainer_cfg:
    max_epochs: 5000
    gpus: 2
    num_nodes: ${nodes}
    distributed_backend: ddp
    #train_percent_check: 0.1

train_set_cfg:
    _target_: src.datasets.CovidDataset
    data_path: /home/li25662/AIforGreatGood/biotransfer/data/covid/
    split: train
    chain: 14L
    average_replicates: True
    add_static_ends: True
    correction: replicate
    filter_nan: median
    
train_dataloader_cfg:
    batch_size: 32
    shuffle: True

callback_cfgs:
    early_stopping:
        _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: val_loss
        verbose: True
        patience: 20
        
checkpoint_callback_cfg:
    filepath: "checkpoint/"
    
val_set_cfg:
    _target_: src.datasets.CovidDataset
    data_path: /home/li25662/AIforGreatGood/biotransfer/data/covid/
    split: valid
    chain: 14L
    average_replicates: True
    add_static_ends: True
    correction: replicate
    filter_nan: median
    
val_dataloader_cfg:
    batch_size: 32

