defaults:
    - logger_cfgs: Tensorboard
    #- hydra/launcher: submitit_slurm

experiment_name: "eval_bertlm_pfam"
nodes: 1
reload_checkpoint_path: <Full Path>/pfam.ckpt # Full path to the pretrained model checkpoint

hydra:
    run:
        dir: ./results/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

model_cfg: 
    _target_: src.models.BertForMaskedLanguageModeling
    model_type: transformer
    task: masked_language_modeling
    model_config_file: <Full Path>/biotransfer/pretrained_models/bert_configs/config.json # full path to the bert config file
    lr: 1e-5
    warmup_steps: 10000
    
trainer_cfg:
    gpus: 2
    num_nodes: ${nodes}
    distributed_backend: ddp

eval_set_cfg:
    _target_: src.datasets.PfamLanguageModelingDataset
    data_path: <Full Path>/biotransfer/data/pfam/pfam_holdout.lmdb
    tokenizer: "iupac"
    in_memory: False
    maxlen: 512
    
eval_dataloader_cfg:
    batch_size: 32
