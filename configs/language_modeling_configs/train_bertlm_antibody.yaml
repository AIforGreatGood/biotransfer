defaults:
    - logger_cfgs: Tensorboard
    #- hydra/launcher: submitit_slurm

experiment_name: "train_language_modeling_on_antibody"
nodes: 1
#reload_checkpoint_path: "path to model here"

hydra:
    run:
        dir: ./results/train/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

model_cfg: 
    _target_: src.models.BertForMaskedLanguageModeling
    model_type: transformer
    task: masked_language_modeling
    model_config_file: <Full Path>/biotransfer/pretrained_models/bert_configs/config.json # full path to the bert config file
    lr: 1e-5
    warmup_steps: 10000

trainer_cfg:
    max_epochs: 10
    gpus: 2
    num_nodes: ${nodes}
    distributed_backend: ddp
    accumulate_grad_batches: 256 # set up enable an effective batch_size=1024 for a single 2-GPU node system
    #precision: 16 #Precision and amp_level are used to speed up the training with a small cost to model performance; install apex to enable mix-precision computing
    #amp_level: 'O2'
   

train_set_cfg:
    _target_: src.datasets.AntibodyLanguageModelingDataset
    data_path: /home/gridsan/groups/ai4bio_shared/datasets/oas/heavy_chain/lmdb/oas_heavy_train.lmdb
    tokenizer: 'iupac'
    in_memory: False
    maxlen: 512
    
train_dataloader_cfg:
    batch_size: 8
    num_workers: 8
    shuffle: True

callback_cfgs:
    early_stopping:
        _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: val_loss
        verbose: True
        
checkpoint_callback_cfg:
    filepath: "checkpoint/"
    
val_set_cfg:
    _target_: src.datasets.AntibodyLanguageModelingDataset
    data_path: /home/gridsan/groups/ai4bio_shared/datasets/oas/heavy_chain/lmdb/oas_heavy_valid.lmdb
    tokenizer: 'iupac'
    in_memory: False
    maxlen: 512
    
val_dataloader_cfg:
    batch_size: 8
    num_workers: 8
