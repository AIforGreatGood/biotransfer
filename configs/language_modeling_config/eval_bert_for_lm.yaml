defaults:
    - model_cfg: BertLM
    - logger_cfgs: Tensorboard
    - hydra/launcher: submitit_slurm

experiment_name: "bert_pfam_plus_heavy_chain_full_${nodes}_eval"
nodes: 1
reload_checkpoint_path: "/home/gridsan/groups/ai4bio_shared/models/in_house/heavychain_initialized_by_pfam_batchsize_2048_layers_24_lr_1e-5_hidsize_1024_intermediatesize_4096_attentionheads_16_epoch_5.ckpt"

hydra:
    run:
        dir: ./results_bert_batch/eval/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

    sweep:
        dir: ./results/train/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
        
    launcher:
        partition: gaia
        #gpus_per_node: 2
        tasks_per_node: 2
        nodes: ${nodes}
        constraint: xeon-g6
        timeout_min: 10000
        additional_parameters:
            {'gres':'gpu:volta:2'}
############

trainer_cfg:
    max_epochs: 10
    gpus: 2
    num_nodes: ${nodes}
    distributed_backend: ddp
    precision: 16
    amp_level: 'O2'

eval_set_cfg:
    _target_: src.datasets.AntibodyBertLanguageModelingDataset
    data_path: "/home/gridsan/groups/ai4bio_shared/datasets/oas/heavy_chain/lmdb"
    split: test
    tokenizer: "iupac"
    in_memory: False
    maxlen: 512
    
eval_dataloader_cfg:
    batch_size: 32
